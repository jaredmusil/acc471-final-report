# Results

## Random Forest

First we decided to use a random forest models to get a rough approximation of what input factors were important. Note that this type of analysis tends to favor factors with high numbers of levels. We did not try and correct for this as it is more of a starting point in understanding our dataset for use in the below models. 

```{r random-forest, echo=FALSE, message=FALSE, warning=FALSE}
data_clean <- read.csv("data/data-clean.csv", header = TRUE)
set.seed(100)
sample_size <- floor(0.5 * nrow(data_clean))
training_indexs <- sample(seq_len(nrow(data_clean)), size = sample_size)
training_data <- data_clean[training_indexs, ]
test_data <- data_clean[-training_indexs, ]

library(randomForest)
library(caret)
library(rpart)
model_random_forest = randomForest(symboling~., ntree = 250, data = training_data)

factor_importance_plot = varImpPlot(model_random_forest)
```

As can be seen from the plot, `make`, `num_of_doors`, and `wheel_base` all showed high relative importance to the `symboling` classification factor. The relative drop off in relative importance prompted us to limit our inquiry into just these three predictors for the next model.

## Linear Regression

Armed with the relative factor importance, we first tried a few variations of simple linear regression models with factors that we thought would be important, such as:

* `make` alone.
* `num_of_doors` alone.
* `make` and `num_of_doors`.
* `make`, `numofdoors`, and `wheelbase`.

Interestingly, this analysis determined that the car manufactur Volvo is significantly safer that its peers. Additionally cars with two doors are also much more likely to not be safe than those with four doors. 

```{r linear-regression, echo=TRUE, warning=FALSE}
# Linear Regression model
lm_make                      <- lm(symboling~make, data = training_data)
lm_numofdoors                <- lm(symboling~num_of_doors, data = training_data)
lm_make_numofdoors           <- lm(symboling~make + num_of_doors, data = training_data)
lm_make_numofdoors_wheelbase <- lm(symboling~make + num_of_doors + wheel_base, data = training_data)

# P Values
p_make                      <- predict(lm_make, data = test_data)
p_numofdoors                <- predict(lm_numofdoors, data = test_data)
p_make_numofdoors           <- predict(lm_make_numofdoors, data = test_data)
p_make_numofdoors_wheelbase <- predict(lm_make_numofdoors_wheelbase, data = test_data)

# Root Mean Square Error
rmse_make                      <- sqrt(mean((p_make - test_data$symboling)^2))
rmse_numofdoors                <- sqrt(mean((p_numofdoors - test_data$symboling)^2))
rmse_make_numofdoors           <- sqrt(mean((p_make_numofdoors - test_data$symboling)^2))
rmse_make_numofdoors_wheelbase <- sqrt(mean((p_make_numofdoors_wheelbase - test_data$symboling)^2))

# Display summary & RMSE results
summary(lm_make)
rmse_make

summary(lm_numofdoors)
rmse_numofdoors

summary(lm_make_numofdoors)
rmse_make_numofdoors

summary(lm_make_numofdoors_wheelbase)
rmse_make_numofdoors_wheelbase
```

## Classification Tree

```{r classification-tree, echo=TRUE}
# import classification tree library
library(rpart)

# load data
data_clean <- read.csv("data/data-clean.csv")

# grow tree 
tree <- rpart(symboling ~ normalized_losses + make + fuel_type + aspiration + 
               num_of_doors + body_style + drive_wheels + engine_location + 
               wheel_base + length + width + height + curb_weight + 
               engine_type + num_of_cylinders + engine_size + fuel_system + 
               bore + stroke + compression_ratio + horsepower + peak_rpm + 
               city_mpg + highway_mpg + price, method="class", data=data_clean)
# NOTE: method={"class" for a classification tree, "anova" for a regression tree}

# plot tree 
# plot(fit, uniform=TRUE, main="Classification Tree")
# text(fit, use.n=TRUE, all=TRUE, cex=.8)
```

```{r regression-tree-pruned, echo=TRUE}
# prune the tree 
pfit <- prune(tree, cp=0.01160389) # from cptable   

# plot the pruned tree 
plot(pfit, uniform=TRUE, main="Pruned Classification Tree")
text(pfit, use.n=TRUE, all=TRUE, cex=.8)
```


