# Results

...

## Random Forest

First we decided to use a random forest models to get a rough approximation of what input factors were important. Note that this type of analysis tends to favor factors with high numbers of levels. We did not try and correct for this as it is more of a starting point in understanding our dataset for use in the below models. 

```{r random-forest, echo=FALSE, message=FALSE, warning=FALSE}
data_clean <- read.csv("data/data-clean.csv", header = TRUE)
set.seed(100)
sample_size <- floor(0.5 * nrow(data_clean))
training_indexs <- sample(seq_len(nrow(data_clean)), size = sample_size)
training_data <- data_clean[training_indexs, ]
test_data <- data_clean[-training_indexs, ]

library(randomForest)
library(caret)
library(rpart)
model_random_forest = randomForest(symboling~., ntree = 250, data = training_data)

factor_importance_plot = varImpPlot(model_random_forest)
```

As can be seen from the plot, `make`, `num_of_doors`, and `wheel_base` all showed high relative importance to the `symboling` classification factor. The relative drop off in relative importance prompted us to limit our inquiry into just these three predictors for the next model.

## Linear Regression

Armed with the relative factor importance, we first tried a few variations of simple linear regression models with factors that we thought would be important, such as:

* `make` alone.
* `num_of_doors` alone.
* `make` and `num_of_doors`.
* `make`, `numofdoors`, and `wheelbase`.
* all variables included

```{r linear-regression, echo=FALSE, warning=FALSE}
# Linear Regression model
lm_make                      <- lm(symboling~make, data = training_data)
lm_numofdoors                <- lm(symboling~num_of_doors, data = training_data)
lm_make_numofdoors           <- lm(symboling~make + num_of_doors, data = training_data)
lm_make_numofdoors_wheelbase <- lm(symboling~make + num_of_doors + wheel_base, data = training_data)
#lm_all <- lm(symboling~., data = training_data)

# P Values
p_make                      <- predict(lm_make, data = test_data)
p_numofdoors                <- predict(lm_numofdoors, data = test_data)
p_make_numofdoors           <- predict(lm_make_numofdoors, data = test_data)
p_make_numofdoors_wheelbase <- predict(lm_make_numofdoors_wheelbase, data = test_data)

# Root Mean Square Error
rmse_make                      <- sqrt(mean((p_make - test_data$symboling)^2))
rmse_numofdoors                <- sqrt(mean((p_numofdoors - test_data$symboling)^2))
rmse_make_numofdoors           <- sqrt(mean((p_make_numofdoors - test_data$symboling)^2))
rmse_make_numofdoors_wheelbase <- sqrt(mean((p_make_numofdoors_wheelbase - test_data$symboling)^2))

# Display summary & RMSE results
summary(lm_make)
print("RMSE: ", rmse_make)

summary(lm_numofdoors)
print("RMSE: ", rmse_numofdoors)

summary(lm_make_numofdoors)
print("RMSE: ", rmse_make_numofdoors)

summary(lm_make_numofdoors_wheelbase)
print("RMSE: ", rmse_make_numofdoors_wheelbase)

```

## Regression Tree

```{r regression-tree, echo=FALSE}
# import regression tree library
library(rpart)

# load data
data_clean <- read.csv("data/data-clean.csv")

# grow tree 
fit <- rpart(symboling ~ normalized_losses + make + fuel_type + aspiration + 
               num_of_doors + body_style + drive_wheels + engine_location + 
               wheel_base + length + width + height + curb_weight + 
               engine_type + num_of_cylinders + engine_size + fuel_system + 
               bore + stroke + compression_ratio + horsepower + peak_rpm + 
               city_mpg + highway_mpg + price, method="class", data=data_clean)
# NOTE: method={"class" for a classification tree, "anova" for a regression tree}

# plot tree 
plot(fit, uniform=TRUE, main="Classification Tree")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
```

```{r regression-tree-pruned, echo=FALSE}
# prune the tree 
pfit <- prune(fit, cp=0.01160389) # from cptable   

# plot the pruned tree 
plot(pfit, uniform=TRUE, main="Pruned Regression Tree")
text(pfit, use.n=TRUE, all=TRUE, cex=.8)
```

...

**Lift Chart**

...

**Decile Chart**

...

## Classification Tree

**Lift Chart**

...

**Decile Chart**

...
